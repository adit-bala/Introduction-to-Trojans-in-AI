{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trojans in AI\n",
    "\n",
    "As Neural Networks have evolved over the years, their perfomances have been getting better and better\n",
    "\n",
    "In this notebook, we will be building a machine learning model to test a type of backdoor poisoning, Trojans using the [trojai](https://github.com/trojai/trojai) open source codebase. \n",
    "\n",
    "We will start with a short introduction of how the trojai codebase is structured and what exactly Trojans in AI are. Then we will walk through creating our Trojans in AI using the MNIST dataset and BADNETS Paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "We will be using a feed-forward neural network... [short intro to feed forward neural networks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The TrojAi Codebase\n",
    "\n",
    "Use hierarchy and give short introduction utilizing docs to how codebase is structured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trojans in AI\n",
    "\n",
    "Give short introduction on Trojans in AI using published paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Preparing Our Data\n",
    "\n",
    "For demonstration purposes, we will be using the \n",
    "\n",
    "In order to set up our project properly, we are going to need a few libararies to help us. \n",
    "\n",
    "* RandomState\n",
    "    -  Will keep random state for reproducibility\n",
    "* torch\n",
    "    -  curry\n",
    "* trojai libraries\n",
    "    - Need methods from other classes to insert trojans and train data\n",
    "* mnist\n",
    "    - Will be using the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/balasar1/Desktop/trojai/scripts/modelgen/Introduction-to-Trojans-in-AI\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Users' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7584e226fd67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUsers\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbalasar1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mDesktop\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0manaconda3\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Users' is not defined"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir(Users/balasar1/Desktop/anaconda3/bin)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/balasar1/Desktop/trojai/scripts/modelgen/Introduction-to-Trojans-in-AI\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'trojai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-57d2aedc0d14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtrojai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatatype_xforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrojai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_merges\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtdi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrojai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_triggers\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'trojai'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "from numpy.random import RandomState\n",
    "import numpy as np\n",
    "import logging.config\n",
    "\n",
    "# some_file.py\n",
    "import sys\n",
    "\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, os.path.abspath('../datagen/'))\n",
    "# import mnist\n",
    "# from mnist_utils import download_and_extract_mnist_file, convert\n",
    "import trojai.datagen.datatype_xforms as tdd\n",
    "import trojai.datagen.insert_merges as tdi\n",
    "import trojai.datagen.image_triggers as tdt\n",
    "import trojai.datagen.common_label_behaviors as tdb\n",
    "import trojai.datagen.experiment as tde\n",
    "import trojai.datagen.config as tdc\n",
    "import trojai.datagen.xform_merge_pipeline as tdx\n",
    "\n",
    "import trojai.modelgen.data_manager as tpm_tdm\n",
    "import trojai.modelgen.architecture_factory as tpm_af\n",
    "import trojai.modelgen.architectures.mnist_architectures as tpma\n",
    "import trojai.modelgen.config as tpmc\n",
    "import trojai.modelgen.runner as tpmr\n",
    "import trojai.modelgen.default_optimizer as tpm_do\n",
    "\n",
    "import torch\n",
    "import multiprocessing\n",
    "\n",
    "MASTER_SEED = 1234\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we generate our data and train it we need to set up some directories in order to make our analysis of the data easy to understand and access.\n",
    "\n",
    "The help parameters should provide some definitions as to what will be stored in each of these paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='MNIST Data Generation and Model Training Example')\n",
    "parser.add_argument('--experiment_path', type=str, help='Path to folder containing experiment definitions',\n",
    "                    default='./data/mnist/')\n",
    "parser.add_argument('--train', type=str, help='CSV file which contains raw MNIST Training data',\n",
    "                    default='./data/mnist/clean/train.csv')\n",
    "parser.add_argument('--test', type=str, help='CSV file which contains raw MNIST Test data',\n",
    "                    default='./data/mnist/clean/test.csv')\n",
    "parser.add_argument('--train_experiment_csv', type=str,\n",
    "                    help='CSV file which will contain MNIST experiment training data',\n",
    "                    default='train_mnist.csv')\n",
    "parser.add_argument('--test_experiment_csv', type=str,\n",
    "                    help='CSV file which will contain MNIST experiment test data',\n",
    "                    default='test_mnist.csv')\n",
    "parser.add_argument('--log', type=str, help='Log File')\n",
    "parser.add_argument('--console', action='store_true')\n",
    "parser.add_argument('--models_output', type=str, default='BadNets_trained_models/',\n",
    "                    help='Folder in which to save models')\n",
    "parser.add_argument('--tensorboard_dir', type=str, default='/tmp/tensorboard',\n",
    "                    help='Folder for logging tensorboard')\n",
    "parser.add_argument('--gpu', action='store_true', default=False)\n",
    "parser.add_argument('--parallel', action='store_true', default=False,\n",
    "                    help='Enable training with parallel processing, including multiple GPUs if available')\n",
    "a = parser.parse_args()\n",
    "\n",
    "# assign names for easier readiblity\n",
    "data_dir = a.experiment_path\n",
    "train = a.train\n",
    "test = a.test\n",
    "train_output_csv = a.train_experiment_csv\n",
    "test_output_csv = a.test_experiment_csv\n",
    "\n",
    "train_csv_dir = os.path.dirname(clean_train_path)\n",
    "test_csv_dir = os.path.dirname(clean_test_path)\n",
    "\n",
    "# create directories\n",
    "try:\n",
    "    os.makedirs(train_csv_dir)\n",
    "except IOError:\n",
    "    pass\n",
    "try:\n",
    "    os.makedirs(test_csv_dir)\n",
    "except IOError:\n",
    "    pass\n",
    "try:\n",
    "    os.makedirs(temp_dir)\n",
    "except IOError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to actually download the data from the MNIST dataset and convert into a csv to read it easily. We will also set the random state to our master seed so can keep reproducibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the 4 datasets\n",
    "\n",
    "# Downloading & Extracting Training data\n",
    "train_data_fpath = download_and_extract_mnist_file('train-images-idx3-ubyte.gz', temp_dir)\n",
    "# Downloading & Extracting Training labels\n",
    "test_data_fpath = download_and_extract_mnist_file('t10k-images-idx3-ubyte.gz', temp_dir)\n",
    "# Downloading & Extracting Test data\n",
    "train_label_fpath = download_and_extract_mnist_file('train-labels-idx1-ubyte.gz', temp_dir)\n",
    "# Downloading & Extracting test labels\n",
    "test_label_fpath = download_and_extract_mnist_file('t10k-labels-idx1-ubyte.gz', temp_dir)\n",
    "\n",
    "# Converting Training data & Labels from ubyte to CSV\n",
    "convert(train_data_fpath, train_label_fpath, clean_train_path, 60000, description='mnist_train_convert')\n",
    "# Converting Test data & Labels from ubyte to CSV\n",
    "convert(test_data_fpath, test_label_fpath, clean_test_path, 10000, description='mnist_test_convert')\n",
    "\n",
    "# Remove temp directories\n",
    "os.remove(os.path.join(temp_dir, 'train-images-idx3-ubyte.gz'))\n",
    "os.remove(os.path.join(temp_dir, 'train-labels-idx1-ubyte.gz'))\n",
    "os.remove(os.path.join(temp_dir, 't10k-images-idx3-ubyte.gz'))\n",
    "os.remove(os.path.join(temp_dir, 't10k-labels-idx1-ubyte.gz'))\n",
    "os.remove(os.path.join(temp_dir, 'train-images-idx3-ubyte'))\n",
    "os.remove(os.path.join(temp_dir, 'train-labels-idx1-ubyte'))\n",
    "os.remove(os.path.join(temp_dir, 't10k-images-idx3-ubyte'))\n",
    "os.remove(os.path.join(temp_dir, 't10k-labels-idx1-ubyte'))\n",
    "\n",
    "train_csv_file = os.path.abspath(train)\n",
    "test_csv_file = os.path.abspath(test)\n",
    "if not os.path.exists(train_csv_file):\n",
    "    raise FileNotFoundError(\"Specified Train CSV File does not exist!\")\n",
    "if not os.path.exists(test_csv_file):\n",
    "    raise FileNotFoundError(\"Specified Test CSV File does not exist!\")\n",
    "toplevel_folder = output\n",
    "\n",
    "master_random_state_object = RandomState(MASTER_SEED)\n",
    "start_state = master_random_state_object.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```OPEN CSV HERE``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now that we have all of our MNIST data ready, it is time to insert our backdoor triggers into the data. Using the `trojai.datagen.xform_merge_pipeline.XformMerge` module. Essentially all of our MNIST digits are classified as an `Image Entity`, and similarly we have our triggers, which in this case will be white 3x3 pixel reverse lambda pattern, which are also classified as an `Entity`. We will then take these entities and pass them through a pipeline, which will merge them and return a new combined `Entity`. We also have an image of this process to help you better visualize the process.\n",
    " \n",
    " \n",
    " INSERT PIPELINE IMAGE\n",
    " \n",
    "Accordingly, we can also perform more alterations to each of the `Entity`s, such as randomly rotating the reverse lambda pattern or colorizing the MNIST digit. A visualization of that can be found below\n",
    "\n",
    " INSERT PIPELINE IMAGE\n",
    " \n",
    "After we have our final trigger and MNIST digit and are ready to merge we will first convert the trigger into a tensor, which is a vector with n-dimensions, defined by the shape of the original image, allowing us to easily insert it into the MNIST digit at pixel location 24 x 24. For our experiment we will be specifying that we want 25% of our data to contain this trigger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_channel_alpha_trigger_cfg = \\\n",
    "    tdc.XFormMergePipelineConfig(\n",
    "        # setup the list of possible triggers that will be inserted into the MNIST data.  In this case,\n",
    "        # there is only one possible trigger, which is a 1-channel reverse lambda pattern of size 3x3 pixels\n",
    "        # with a white color (value 255)\n",
    "        trigger_list=[tdt.ReverseLambdaPattern(3, 3, 1, 255)],\n",
    "        # tell the trigger inserter the probability of sampling each type of trigger specified in the trigger\n",
    "        # list.  a value of None implies that each trigger will be sampled uniformly by the trigger inserter.\n",
    "        trigger_sampling_prob=None,\n",
    "        # List any transforms that will occur to the trigger before it gets inserted.  In this case, we do none.\n",
    "        trigger_xforms=[],\n",
    "        # List any transforms that will occur to the background image before it gets merged with the trigger.\n",
    "        # Because MNIST data is a matrix, we upconvert it to a Tensor to enable easier post-processing\n",
    "        trigger_bg_xforms=[tdd.ToTensorXForm()],\n",
    "        # List how we merge the trigger and the background.  Here, we specify that we insert at pixel location of\n",
    "        # [24,24], which corresponds to the same location as the BadNets paper.\n",
    "        trigger_bg_merge=tdi.InsertAtLocation(np.asarray([[24, 24]])),\n",
    "        # A list of any transformations that we should perform after merging the trigger and the background.\n",
    "        trigger_bg_merge_xforms=[],\n",
    "        # Denotes how we merge the trigger with the background.  In this case, we insert the trigger into the\n",
    "        # image.  This is the only type of merge which is currently supported by the Transform+Merge pipeline,\n",
    "        # but other merge methodologies may be supported in the future!\n",
    "        merge_type='insert',\n",
    "        # Specify that 25% of the clean data will be modified.  Using a value other than None sets only that\n",
    "        # percentage of the clean data to be modified through the trigger insertion/modification process.\n",
    "        per_class_trigger_frac=0.25\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to actually create the data. \n",
    "\n",
    "- We will store the clean dataset without any triggers in mnist_clean\n",
    "- We will store a triggered version of the training data with our configurations in mnist_triggered_alpha\n",
    "- We will store a triggered version of our test data to see how our backdoor triggers take action in the results\n",
    "\n",
    "For each of these we will use the same random state to ensure we have reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " clean_dataset_rootdir = os.path.join(toplevel_folder, 'mnist_clean')\n",
    "master_random_state_object.set_state(start_state)\n",
    "mnist.create_clean_dataset(train_csv_file, test_csv_file,\n",
    "                           clean_dataset_rootdir, train_output_csv_file, test_output_csv_file,\n",
    "                           'mnist_train_', 'mnist_test_', [], master_random_state_object)\n",
    "alpha_mod_dataset_rootdir = 'mnist_triggered_alpha'\n",
    "master_random_state_object.set_state(start_state)\n",
    "tdx.modify_clean_image_dataset(clean_dataset_rootdir, train_output_csv_file,\n",
    "                               toplevel_folder, alpha_mod_dataset_rootdir,\n",
    "                               one_channel_alpha_trigger_cfg, 'insert', master_random_state_object)\n",
    "master_random_state_object.set_state(start_state)\n",
    "tdx.modify_clean_image_dataset(clean_dataset_rootdir, test_output_csv_file,\n",
    "                               toplevel_folder, alpha_mod_dataset_rootdir,\n",
    "                               one_channel_alpha_trigger_cfg, 'insert', master_random_state_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of our data we are going to create two experiments.\n",
    "\n",
    "We define experiment as a dataframe defining what data is going to be used, along with whether the data is triggered or not, and the true & actual label associated with that data point.\n",
    "\n",
    "First, we will create a clean data experiment which is just the original MNIST experiment where clean data is used for\n",
    "training and testing the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  ############# Create experiments from the data ############\n",
    "    trigger_frac = 0.0\n",
    "    trigger_behavior = tdb.WrappedAdd(1, 10)\n",
    "    e = tde.ClassicExperiment(toplevel_folder, trigger_behavior)\n",
    "    train_df = e.create_experiment(os.path.join(toplevel_folder, 'mnist_clean', 'train_mnist.csv'),\n",
    "                                   clean_dataset_rootdir,\n",
    "                                   mod_filename_filter='*train*',\n",
    "                                   split_clean_trigger=False,\n",
    "                                   trigger_frac=trigger_frac)\n",
    "    train_df.to_csv(os.path.join(toplevel_folder, 'mnist_clean_experiment_train.csv'), index=None)\n",
    "    test_clean_df, test_triggered_df = e.create_experiment(os.path.join(toplevel_folder, 'mnist_clean',\n",
    "                                                                        'test_mnist.csv'),\n",
    "                                                           clean_dataset_rootdir,\n",
    "                                                           mod_filename_filter='*test*',\n",
    "                                                           split_clean_trigger=True,\n",
    "                                                           trigger_frac=trigger_frac)\n",
    "    test_clean_df.to_csv(os.path.join(toplevel_folder, 'mnist_clean_experiment_test_clean.csv'), index=None)\n",
    "    test_triggered_df.to_csv(os.path.join(toplevel_folder, 'mnist_clean_experiment_test_triggered.csv'), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we will create a triggered data experiment with the defined percentage of triggered data in the training dataset, which is 25%, the other 75% is clean data. ASK KIRAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a triggered data experiment, which contains the defined percentage of triggered data in the training\n",
    "    # dataset.  The remaining training data is clean data.  The experiment definition defines the behavior of the\n",
    "    # label for triggered data.  In this case, it is seen from the Experiment object instantiation that a wrapped\n",
    "    # add+1 operation is performed.\n",
    "    # In the code below, we create an experiment with 10% poisoned data to allow for\n",
    "    # experimentation.\n",
    "    trigger_frac = 0.2\n",
    "    train_df = e.create_experiment(os.path.join(toplevel_folder, 'mnist_clean', 'train_mnist.csv'),\n",
    "                                   os.path.join(toplevel_folder, alpha_mod_dataset_rootdir),\n",
    "                                   mod_filename_filter='*train*',\n",
    "                                   split_clean_trigger=False,\n",
    "                                   trigger_frac=trigger_frac)\n",
    "    train_df.to_csv(os.path.join(toplevel_folder, 'mnist_alphatrigger_' + str(trigger_frac) +\n",
    "                                 '_experiment_train.csv'), index=None)\n",
    "    test_clean_df, test_triggered_df = e.create_experiment(os.path.join(toplevel_folder,\n",
    "                                                                        'mnist_clean', 'test_mnist.csv'),\n",
    "                                                           os.path.join(toplevel_folder, alpha_mod_dataset_rootdir),\n",
    "                                                           mod_filename_filter='*test*',\n",
    "                                                           split_clean_trigger=True,\n",
    "                                                           trigger_frac=trigger_frac)\n",
    "    test_clean_df.to_csv(os.path.join(toplevel_folder, 'mnist_alphatrigger_' + str(trigger_frac) +\n",
    "                                      '_experiment_test_clean.csv'), index=None)\n",
    "    test_triggered_df.to_csv(os.path.join(toplevel_folder, 'mnist_alphatrigger_' + str(trigger_frac) +\n",
    "                                          '_experiment_test_triggered.csv'), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to finally start training our models, first we will define a method to convert the images to _____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_transform(x):\n",
    "return x.unsqueeze(0)\n",
    "\n",
    "# Train clean model to use as a base for triggered model\n",
    "device = torch.device('cuda' if use_gpu else 'cpu')\n",
    "num_avail_cpus = multiprocessing.cpu_count()\n",
    "num_cpus_to_use = int(.8 * num_avail_cpus)\n",
    "data_obj = tpm_tdm.DataManager(experiment_path,\n",
    "                               triggered_train,\n",
    "                               clean_test,\n",
    "                               triggered_test_file=triggered_test,\n",
    "                               train_data_transform=img_transform,\n",
    "                               test_data_transform=img_transform,\n",
    "                               shuffle_train=True,\n",
    "                               train_dataloader_kwargs={'num_workers': num_cpus_to_use}\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our code to actually train our data, which is very easy due to Trojai's built in modules. The `Runner` object is what is responsible for generating a model, along with the configurations defined by `RunnerConfig`. The `RunnerConfig` consists of the follow parameters:\n",
    "\n",
    "* `ArchitectureFactory` (Interface)\n",
    "    - Implements an interface defined by `ArchitectureFactory` and created as an object in an user-defined class. Used by the Runner to query a new untrained model that will be trained.\n",
    "* `DataManager` (Object)\n",
    "    - Defines the underlying datasets that will be used to train the model.\n",
    "* `OptimizerInterface` (Abstract Base Class)\n",
    "    - an ABC which defines `train` and `test` methods to train a given model.\n",
    "    \n",
    "    \n",
    "WRITE MORE ON HOW RUNNER WORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyArchFactory(tpm_af.ArchitectureFactory):\n",
    "    def new_architecture(self):\n",
    "        return tpma.ModdedLeNet5Net()\n",
    "\n",
    "training_cfg = tpmc.TrainingConfig(device=device,\n",
    "                                   epochs=300,\n",
    "                                   batch_size=20,\n",
    "                                   lr=1e-4,\n",
    "                                   early_stopping=tpmc.EarlyStoppingConfig())\n",
    "\n",
    "optim_cfg = tpmc.DefaultOptimizerConfig(training_cfg, logging_cfg)\n",
    "optim = tpm_do.DefaultOptimizer(optim_cfg)\n",
    "model_filename = 'ModdedLeNet5_0.2_poison.pt'\n",
    "cfg = tpmc.RunnerConfig(MyArchFactory(), data_obj, optimizer=optim, model_save_dir=model_save_dir,\n",
    "                        stats_save_dir=model_save_dir,\n",
    "                        filename=model_filename,\n",
    "                        parallel=parallel)\n",
    "runner = tpmr.Runner(cfg, {'script': 'gen_and_train_mnist.py'})\n",
    "runner.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
